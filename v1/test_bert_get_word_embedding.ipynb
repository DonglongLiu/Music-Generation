{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试Transformer\n",
    "from transformers import BertModel, BertTokenizer\n",
    "import torch\n",
    "\n",
    "# 加载预训练的BERT模型和分词器\n",
    "model_name = './pre_train_model/bert'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "# 输入文本\n",
    "text = ('怎麼這城市裡到處流行破碎戀情', '故意挑剔生氣', '不知不覺分開了又忘記後', '夜一對')\n",
    "text = ('')\n",
    "\n",
    "def get_embedding(text):\n",
    "# 使用分词器对文本进行分词\n",
    "    tokens = tokenizer.tokenize(tokenizer.decode(tokenizer.encode(text)))\n",
    "\n",
    "    # 将分词转换为模型的输入格式\n",
    "    inputs = tokenizer.encode(text, max_length=22, padding='max_length', truncation=True, return_tensors=\"pt\")\n",
    "    # print(inputs.shape)\n",
    "\n",
    "    # 获取模型的输出，包括词向量\n",
    "    outputs = model(inputs)\n",
    "\n",
    "    # 提取最后一层的隐藏状态\n",
    "    last_hidden_states = outputs.last_hidden_state\n",
    "\n",
    "    # 获取每个分词的词向量\n",
    "    word_embeddings = last_hidden_states[0]\n",
    "    # attention_mask = inputs['attention_mask']\n",
    "\n",
    "    # # 使用平均池化操作获取固定长度的输出向量\n",
    "    # pooled_output = torch.sum(last_hidden_state * attention_mask.unsqueeze(-1), dim=1) / attention_mask.sum(dim=1, keepdim=True)\n",
    "\n",
    "    return word_embeddings\n",
    "\n",
    "for i in range(len(text)):\n",
    "    print(get_embedding(text[i]).shape)\n",
    "# print(get_embedding(text[-1]))\n",
    "# # 选择特定位置的词向量（例如，第一个词的词向量）\n",
    "# first_word_embedding = word_embeddings[0]\n",
    "\n",
    "# # 将 PyTorch Tensor 转换为 NumPy array\n",
    "# first_word_embedding_np = first_word_embedding.detach().numpy()\n",
    "\n",
    "# print(f\"Original text: {text}\")\n",
    "# print(f\"Tokens: {tokens}\")\n",
    "# print(f\"First word embedding shape: {first_word_embedding.shape}\")\n",
    "# # print(f\"First word embedding: {first_word_embedding_np}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[101, 100, 100, 100, 100, 102]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 768])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fig. 3 | Country-level clusters. a, Hierarchical cluster of countries based  on average marginal causal effect. One hundred and thirty countries with  at least 100 respondents were selected (range, 101–448,125). The three  colours of the dendrogram branches represent three large clusters—Western,  Eastern, and Southern. Country names are coloured according to the  Inglehart–Welzel Cultural Map 2010–201421. Distributions across the three  clusters reveal stark differences. For instance, cluster 2 (Eastern) consists  mostly of countries of Islamic and Confucian cultures. By contrast, cluster  1 (Western) has large percentages of Protestant, Catholic, and Orthodox  countries in Europe. b, Mean AMCE z-scores of the three major clusters.  Radar plot of the mean AMCE z-scores of three clusters reveals a striking  pattern of differences between the clusters along the nine attributes.  For example, countries belonging to the Southern cluster show a strong  preference for sparing females compared to countries in other clusters\n"
     ]
    }
   ],
   "source": [
    "with open('./1','r') as f:\n",
    "    print(f.read().replace('\\n', ' '))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
